{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Waveform model\n",
    "\n",
    "Build and train model that takes raw waveforms as input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchsummary\n",
    "import torch.nn.functional as F\n",
    "import librosa\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 16000\n",
    "FRAME_LENGTH = SAMPLE_RATE  # 1 second per frame (16000 samples at 16 kHz)\n",
    "FRAME_HOP = FRAME_LENGTH // 2  # 50% overlap between frames\n",
    "\n",
    "class BirdAudioDataset(Dataset):\n",
    "    def __init__(self, root_dir, sample_rate=SAMPLE_RATE, frame_length=FRAME_LENGTH, frame_hop=FRAME_HOP, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.sample_rate = sample_rate\n",
    "        self.frame_length = frame_length\n",
    "        self.frame_hop = frame_hop\n",
    "        self.transform = transform\n",
    "        self.audio_paths = []\n",
    "        self.labels = []\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        \n",
    "        # Load audio file paths and labels\n",
    "        for label in os.listdir(root_dir)[:4]:\n",
    "            label_dir = os.path.join(root_dir, label)\n",
    "            if os.path.isdir(label_dir):\n",
    "                for file_name in os.listdir(label_dir)[:2]:\n",
    "                    if file_name.endswith(\".wav\") or file_name.endswith(\".mp3\"):\n",
    "                        self.audio_paths.append(os.path.join(label_dir, file_name))\n",
    "                        self.labels.append(label)\n",
    "        \n",
    "        # Encode labels (it assumes that in all directories there will be the same directories in the same order)\n",
    "        self.labels = self.label_encoder.fit_transform(self.labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.audio_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        try:\n",
    "            waveform, sr = librosa.load(audio_path, sr=self.sample_rate)  # Resamples to sample_rate if needed\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {audio_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "        max_length = self.sample_rate * 20  # 20 seconds\n",
    "        if waveform.shape[0] > max_length:\n",
    "            waveform = waveform[:max_length]\n",
    "            \n",
    "        waveform = torch.tensor(waveform)\n",
    "\n",
    "        # Ensure the waveform is at least as long as the frame_length\n",
    "        if waveform.shape[0] < self.frame_length:\n",
    "            # Pad or truncate the waveform if it's too short (use padding_value=0)\n",
    "            waveform = F.pad(waveform, (0, self.frame_length - waveform.shape[0]))\n",
    "\n",
    "        # Split waveform into frames with specified length and hop\n",
    "        frames = waveform.unfold(dimension=0, size=self.frame_length, step=self.frame_hop)\n",
    "        \n",
    "        # Apply any additional transform\n",
    "        if self.transform:\n",
    "            frames = self.transform(frames)\n",
    "\n",
    "        return frames, label"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "birddetect",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
