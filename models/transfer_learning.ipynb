{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "import tensorflow_hub as hub\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load yamnet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "yamnet_model_handle = 'https://www.kaggle.com/models/google/yamnet/TensorFlow2/yamnet/1'\n",
    "yamnet_model = hub.load(yamnet_model_handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def load_mp3(filename):\n",
    "    file_content = tf.io.read_file(filename)\n",
    "    wav = tfio.audio.decode_mp3(file_content)\n",
    "    wav = tf.reduce_mean(wav, axis=1)\n",
    "    sample_rate = tfio.audio.AudioIOTensor(filename, dtype=tf.float32).rate\n",
    "    sample_rate = tf.cast(sample_rate, dtype=tf.int64)\n",
    "    wav = tfio.audio.resample(wav, rate_in=sample_rate, rate_out=16000)\n",
    "    return wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = os.path.join('..', 'data', 'xeno-canto')\n",
    "bird_species_df = pd.read_csv('../data/selected_species.csv', sep=',')\n",
    "\n",
    "class_names = []\n",
    "class_id = 0\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "X_val = []\n",
    "y_val = []\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "for dir_name in os.listdir(os.path.join(data_directory, 'train')):\n",
    "    if dir_name.startswith('.'):\n",
    "        continue\n",
    "    class_name = bird_species_df.loc[bird_species_df['Latin name'] == dir_name]['Polish name'].squeeze()\n",
    "    class_names.append(class_name)\n",
    "\n",
    "    for file_name in os.listdir(os.path.join(data_directory, 'train', dir_name))[:1]:\n",
    "        file_path = os.path.join(data_directory, 'train', dir_name, file_name)\n",
    "        X_train.append(file_path)\n",
    "        y_train.append(class_id)\n",
    "\n",
    "    for file_name in os.listdir(os.path.join(data_directory, 'val', dir_name))[:1]:\n",
    "        file_path = os.path.join(data_directory, 'val', dir_name, file_name)\n",
    "        X_val.append(file_path)\n",
    "        y_val.append(class_id)\n",
    "\n",
    "    for file_name in os.listdir(os.path.join(data_directory, 'test', dir_name))[:1]:\n",
    "        file_path = os.path.join(data_directory, 'test', dir_name, file_name)\n",
    "        X_test.append(file_path)\n",
    "        y_test.append(class_id)\n",
    "    \n",
    "    class_id += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['skowronek', 'krzyżówka', 'gęś białoczelna', 'gęś zbożowa', 'jerzyk', 'mewa śmieszka', 'gołąb miejski', 'grzywacz', 'gawron', 'kawka', 'kukułka', 'modraszka', 'oknówka', 'łyska', 'sójka', 'słowik szary', 'słowik rdzawy', 'sroka', 'brzegówka', 'kowalik']\n"
     ]
    }
   ],
   "source": [
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "val_data = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "test_data = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "\n",
    "def load_mp3_for_map(filename, label):\n",
    "    return load_mp3(filename), label\n",
    "\n",
    "train_data = train_data.map(lambda filename, label: load_mp3_for_map(filename, label))\n",
    "val_data = val_data.map(lambda filename, label: load_mp3_for_map(filename, label))\n",
    "test_data = test_data.map(lambda filename, label: load_mp3_for_map(filename, label))\n",
    "\n",
    "def extract_embeddings(wav_data, label):\n",
    "    _, embeddings, _ = yamnet_model(wav_data)\n",
    "    num_embeddings = tf.shape(embeddings)[0]\n",
    "    return (embeddings,\n",
    "            tf.repeat(label, num_embeddings))\n",
    "\n",
    "train_data = train_data.map(extract_embeddings).unbatch()\n",
    "val_data = val_data.map(extract_embeddings).unbatch()\n",
    "test_data = test_data.map(extract_embeddings).unbatch()\n",
    "\n",
    "train_data = train_data.cache().shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "val_data = val_data.cache().batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_data = test_data.cache().batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorSpec(shape=(None, 1024), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))\n"
     ]
    }
   ],
   "source": [
    "print(train_data.element_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_14 (Dense)            (None, 512)               524800    \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 20)                10260     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 535,060\n",
      "Trainable params: 535,060\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(1024), dtype=tf.float32,\n",
    "                          name='input_embedding'),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(class_names))\n",
    "], name='my_model')\n",
    "\n",
    "my_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                 optimizer=\"adam\",\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                            patience=3,\n",
    "                                            restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "65/65 [==============================] - 53s 609ms/step - loss: 1.4962 - accuracy: 0.6142 - val_loss: 2.7907 - val_accuracy: 0.3084\n",
      "Epoch 2/20\n",
      "65/65 [==============================] - 1s 12ms/step - loss: 0.6118 - accuracy: 0.8242 - val_loss: 3.2408 - val_accuracy: 0.2883\n",
      "Epoch 3/20\n",
      "65/65 [==============================] - 1s 13ms/step - loss: 0.4154 - accuracy: 0.8754 - val_loss: 3.3931 - val_accuracy: 0.2913\n",
      "Epoch 4/20\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.2787 - accuracy: 0.9126 - val_loss: 3.6347 - val_accuracy: 0.2993\n"
     ]
    }
   ],
   "source": [
    "history = my_model.fit(train_data,\n",
    "                       epochs=20,\n",
    "                       validation_data=val_data,\n",
    "                       callbacks=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 26s 392ms/step - loss: 2.5558 - accuracy: 0.4279\n",
      "Loss:  2.555769681930542\n",
      "Accuracy:  0.4278582036495209\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = my_model.evaluate(test_data)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main sound is: słowik szary\n"
     ]
    }
   ],
   "source": [
    "testing_file_name = '../data/xeno-canto/test/Luscinia megarhynchos/36984.mp3'\n",
    "wav = load_mp3(testing_file_name)\n",
    "\n",
    "scores, embeddings, spectrogram = yamnet_model(wav)\n",
    "result = my_model(embeddings).numpy()\n",
    "\n",
    "inferred_class = class_names[result.mean(axis=0).argmax()]\n",
    "\n",
    "print(f'The main sound is: {inferred_class}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extend trained model to be able to give it raw wav data as input\n",
    "class ReduceMeanLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, axis=0, **kwargs):\n",
    "    super(ReduceMeanLayer, self).__init__(**kwargs)\n",
    "    self.axis = axis\n",
    "\n",
    "  def call(self, input):\n",
    "    return tf.math.reduce_mean(input, axis=self.axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "saved_model_path = './birds_yamnet.keras'\n",
    "\n",
    "input_segment = tf.keras.layers.Input(shape=(), dtype=tf.float32, name='audio')\n",
    "embedding_extraction_layer = hub.KerasLayer(yamnet_model_handle,\n",
    "                                            trainable=False, name='yamnet')\n",
    "_, embeddings_output, _ = embedding_extraction_layer(input_segment)\n",
    "serving_outputs = my_model(embeddings_output)\n",
    "serving_outputs = ReduceMeanLayer(axis=0, name='classifier')(serving_outputs)\n",
    "serving_model = tf.keras.Model(input_segment, serving_outputs)\n",
    "serving_model.save(saved_model_path, include_optimizer=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "reloaded_model = tf.keras.models.load_model(saved_model_path, custom_objects={'KerasLayer':hub.KerasLayer, 'ReduceMeanLayer': ReduceMeanLayer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main sound is: słowik szary\n"
     ]
    }
   ],
   "source": [
    "reloaded_results = reloaded_model(wav)\n",
    "inferred_class = class_names[tf.math.argmax(reloaded_results)]\n",
    "\n",
    "print(f'The main sound is: {inferred_class}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
